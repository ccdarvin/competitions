{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# working in kaggle\n",
    "\n",
    "kaggle_data_path = '../input'\n",
    "local_data_path = 'data'\n",
    "\n",
    "data_path = kaggle_data_path if os.path.exists(kaggle_data_path) else local_data_path\n",
    "\n",
    "%ls $data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train data\n",
    "data_train = pd.read_csv(f'{data_path}/train.csv')\n",
    "data_test = pd.read_csv(f'{data_path}/test.csv')\n",
    "\n",
    "print('train data shape: ', data_train.shape)\n",
    "print('test data shape: ', data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "\n",
    "y_data = data_train['SalePrice']\n",
    "data_train.drop('SalePrice', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing data and percentage of total\n",
    "data_train.isnull().sum().sort_values(ascending=False)[:10]/len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing data\n",
    "data_test.isnull().sum().sort_values(ascending=False)[:10]/len(data_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "\n",
    "Both training and test data have 6 features with missing values more than 50% of the total number of rows. These features are: `PoolQC`, `MiscFeature`, `Alley`, `Fence`, `MasVnrType`, and `FireplaceQu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be able to work with a single dataset\n",
    "data = pd.concat([data_train, data_test], axis=0)\n",
    "# remove missing data\n",
    "data = data.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'MasVnrType', 'FireplaceQu'], axis=1)\n",
    "# id is index\n",
    "data.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing by type\n",
    "data.isna().sum().groupby(data.dtypes).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three types of features: `int64`, `float64`, and `object`. We will fill the missing values with the mean for `int64` and `float64` features, and with the mode for `object` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data float \n",
    "data_float = data.select_dtypes(include=['float64'])\n",
    "data_float.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete missing data with mean\n",
    "fill_with_mean = data_float[['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageYrBlt', 'GarageArea']].fillna(data_float.mean())\n",
    "# check missing data with median\n",
    "fill_with_median = data_float[['BsmtFullBath', 'BsmtHalfBath', 'GarageCars']].fillna(data_float.median())\n",
    "# update data\n",
    "data_float.update(fill_with_mean)\n",
    "data_float.update(fill_with_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check float missing data\n",
    "data.update(data_float)\n",
    "data_float.isna().sum().sort_values(ascending=False)[:10]/len(data_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data object\n",
    "\n",
    "data_object = data.select_dtypes(include=['object'])\n",
    "data_object.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete missing data with mode\n",
    "data_object = data_object.fillna(data_object.mode().iloc[0])\n",
    "# convert to category\n",
    "data_cat = data_object.astype('category')\n",
    "# updata data\n",
    "data_cat.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odinal features\n",
    "\n",
    "We need to convert ordinal features to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "data_encode = data_cat.apply(le.fit_transform)\n",
    "data_encode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update data\n",
    "data.update(data_encode)\n",
    "\n",
    "# check missing data\n",
    "data.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "\n",
    "data_train = data.loc[data_train['Id']]\n",
    "data_test = data.loc[data_test['Id']]\n",
    "X_test = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(data_train, y_data, test_size=0.2)\n",
    "scores = pd.DataFrame(columns=['train', 'cv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "scores.loc['lenealRegression'] = mean_squared_error(y_train, lr.predict(X_train)), mean_squared_error(y_cv, lr.predict(X_cv))\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "grid_params = {'alpha': alpha}\n",
    "\n",
    "grid_search = GridSearchCV(Lasso(), grid_params, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print('best_params: ', best_params)\n",
    "\n",
    "lasso = Lasso(**best_params)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "scores.loc['lasso'] = mean_squared_error(y_train, lasso.predict(X_train)), mean_squared_error(y_cv, lasso.predict(X_cv))\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(Ridge(), grid_params, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print('best_params: ', best_params)\n",
    "\n",
    "ridge = Ridge(**best_params)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "scores.loc['ridge'] = mean_squared_error(y_train, ridge.predict(X_train)), mean_squared_error(y_cv, ridge.predict(X_cv))\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {'alpha': alpha, 'l1_ratio': alpha}\n",
    "\n",
    "grid_search = GridSearchCV(ElasticNet(), grid_params, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print('best_params: ', best_params)\n",
    "\n",
    "elastic = ElasticNet(**best_params)\n",
    "elastic.fit(X_train, y_train)\n",
    "\n",
    "scores.loc['elastic'] = mean_squared_error(y_train, elastic.predict(X_train)), mean_squared_error(y_cv, elastic.predict(X_cv))\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to csv\n",
    "y_pred = elastic.predict(X_test)\n",
    "pred_df = pd.DataFrame(y_pred, index=data_test.index, columns=['SalePrice'])\n",
    "pred_df.to_csv('data/elastic_submission.csv')\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_child_weight': [1, 2, 3, 4],\n",
    "    'alpha': [0.01, 0.1, 0.5, 1, 10],\n",
    "    'lambda': [0.01, 0.1, 0.5, 1, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBRegressor(), grid_params, cv=5)\n",
    "grid_search.fit(X_train.values, y_train.values)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "xgb_model = XGBRegressor(**best_params)\n",
    "xgb_model.fit(X_train.values, y_train.values)\n",
    "\n",
    "scores.loc['xgb'] = mean_squared_error(y_train, xgb_model.predict(X_train.values)), mean_squared_error(y_cv, xgb_model.predict(X_cv.values))\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph of scores\n",
    "plt.scatter(scores.index, scores['train'], label='train')\n",
    "plt.scatter(scores.index, scores['cv'], label='cv')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(X_test.values)\n",
    "\n",
    "pred_df = pd.DataFrame(y_pred, index=data_test.index, columns=['SalePrice'])\n",
    "pred_df.to_csv('data/xgb_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "model.fit(X_train.values, y_train.values, epochs=100, batch_size=32, validation_data=(X_cv.values, y_cv.values))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
